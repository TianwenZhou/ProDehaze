## ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing

[Paper](https://arxiv.org/abs/2308.10510)|[Project Page](https://zhoutianwen.com/prodehaze)

This is the official repo of ProDehaze: Prompting Diffusion Models Toward Faithful Image Dehazing by Pytorch.
<img src="asset/overview.png" alt="show" style="zoom:90%;" />

News: Our paper has been accepted to ICME 2025!

## Getting started
### Dependency Installation
```shell
# git clone this repository
git clone https://github.com/TianwenZhou/ProDehaze.git
cd ProDehaze

# Create the conda environment 
conda env create --file environment.yaml
conda activate prodehaze

# Install taming & clip
pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers
pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip
pip install -e .
```

### Data Preparation

#### Download
Download train/eval data from the following links:

Training: [*RESIDE*](https://sites.google.com/view/reside-dehaze-datasets/reside-v0)

Testing:
[*I-Haze*](https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/#:~:text=To%20overcome%20this%20issue%20we%20introduce%20I-HAZE%2C%20a,real%20haze%20produced%20by%20a%20professional%20haze%20machine.) / 
[*O-Haze*](https://data.vision.ee.ethz.ch/cvl/ntire18/o-haze/) /
[*Dense-Haze*](https://arxiv.org/abs/1904.02904#:~:text=To%20address%20this%20limitation%2C%20we%20introduce%20Dense-Haze%20-,introducing%20real%20haze%2C%20generated%20by%20professional%20haze%20machines.) /
[*Nh-Haze*](https://data.vision.ee.ethz.ch/cvl/ntire20/nh-haze/) /
[*RTTS*](https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=0) 

#### Training Data Synthesis
To generate the hazy-clear image pair for training, please refer to the augmentation process of [FCB](https://github.com/W-Jilly/frequency-compensated-diffusion-model-pytorch), and prepare the training data in the following structure:

```shell
 #Training data file structure
dataset/RESIDE/
├── HR # ground-truth clear images.
├── HR_hazy_src # hazy images.
└── HR_depth # depth images (Generated by [DepthAnything](https://github.com/DepthAnything/Depth-Anything-V2).

#Testing data (e.g. DenseHaze) file structure
dataset/{name}/
├── HR # ground-truth images.
└── HR_hazy # hazy images.
```
## Pretrained Model

We prepared the pretrained model at:

| Type                                                        | Weights                                        |
| ----------------------------------------------------------- | ------------------------------------------------------------ |
| SPR                                                 | [OneDrive](https://1drv.ms/u/s!AsqtTP8eWS-penA8AqrU8c_I4jU) |
| HCR                                                 | [OneDrive](https://1drv.ms/u/s!AsqtTP8eWS-penA8AqrU8c_I4jU) |

## Inference Pipeline
Using the two pre-trained models above, we can conduct sampling conditioned with a hazy input.
```python
python scripts/sr_val_ddpm_text_T_vqganfin_old.py --config configs/LatentDehazing/v2-finetune_text_T_512.yaml --ckpt CKPT --init-img HAZY_PATH --outdir OUT_PATH --ddpm_steps 50 --dec_w 1 --seed 42 --n_samples 1 --vqgan_ckpt VQGAN_CKPT --colorfix_type none
```
## Training Pipeline
You should train the SPR and HCR seperately.
For the SPR, the training can be conducted with the following command. Note that you have to alter the config files corresponding to your real file paths.
```python
python main.py --train --base configs/LatentDehazing/v2-finetune_text_T_512.yaml --gpus GPU_ID, --name NAME --scale_lr False
```
As for the HCR, you should first generate the additional training data in its .npy format, with the following command, where the CKPT_PATH should be the SPR you have already trained in the first place.
```
python scripts/generate_vqgan_data.py --config configs/LatentDehazing/test_data.yaml --ckpt CKPT_PATH --outdir OUTDIR --skip_grid --ddpm_steps 50 --base_i 0 --seed 10000
```
The generated file structure should look like this:
```
CFW_trainingdata/
    └── inputs
          └── 00000001.png # Hazy input, (512, 512, 3) (resize to 512x512)
          └── ...
    └── gts
          └── 00000001.png # Groundtruth, (512, 512, 3) (512x512)
          └── ...
    └── latents
          └── 00000001.npy # Latent codes (N, 4, 64, 64) 
          └── ...
    └── samples
          └── 00000001.png # Samples decoded directly from the latent codes, just to confirm that the latent codes are correctly generated.
          └── ...
```
After this, you can train the HCR with the following command:
```python
python main.py --train --base configs/autoencoder/autoencoder_kl_64x64x4_resi.yaml --gpus GPU_ID, --name NAME --scale_lr False
```
