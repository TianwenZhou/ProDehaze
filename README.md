# Latent Dehazing

Tianwen Zhou

## Getting Started
### Installation
* This repo is a modification on the [StableSR Repo](https://githubcom/IceClear/StableSR.git)
* Install conda environment
```
cd LatentDehazing
# Create a conda environment and activate
conda env create --file environment.yaml
conda activate stablesr
# Install taming-transformer and clip
pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers
pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip
pip install -e .
```

### Prior Knowledge Branch

#### Training Data
We use RESIDE dataset for training the prior knowledge branch.
```
# Usage of the training set
intern@43.82.40.65:/home/intern/ztw/ztw/ztw/Data/RESIDE_new
# File structure
/RESIDE_new/
├── HR # ground-truth images.
├── HR_hazy_src # hazy images from the OFFICIAL reside dataset
└── HR_depth # depth images (Generated by DepthAnything (https://github.com/LiheYoung/Depth-Anything))

```

#### Training script
Train the prior knowledge branch by finetuning the pre-trained SD-v2.1 model

Before training, modify the ckpt_path in config files ([Line 22](configs/LatentDehazing/v2-finetune_text_512.yaml#L22) and [Line 55](configs/LatentDehazing/v2-finetune_text_512.yaml#L55)) to the following location

Also modify the 'gt_path', 'dataroot_path', 'ref_path' in the [data part of the config file](configs/LatentDehazing/v2-finetune_text_512.yaml#L109) to the location of your ground-truth image, depth images, hazy images
```
# Location of the pre-trained SD-v2.1 model
intern@43.82.40.65:/home/intern/ztw/ztw/ztw/Methods/LatentDehazing/src/stable_diffusion-v2-1_512/v2-1_512-ema-pruned.ckpt
# Training script
# This training script will automatically save model checkpoints in ./logs/NAME
python main,py --train --base configs/LatentDehazing/v2-finetune_text_512.yaml --gpus GPU_ID, --name NAME --scaled_lr False

# If you want to resume training from a specific checkpoint, use
python main,py --train --base configs/LatentDehazing/v2-finetune_text_512.yaml --gpus GPU_ID, --resume CKPT_PATH --scaled_lr False
```

### SAFM (Spatial-Aware Feature Merging) Module Training
#### Data preparation
You have to generate the training data for SAFM using the LDM that you finetuned previously, modify the ckpt_path in config files ([Line 53](configs/LatentDehazingData/test_data.yaml#L53)) with the finetuned LDM 

In our paper, we generated 60k paired training data for SAFM
```
# Generate SAFM Training Data with finetuned LDM_CKPT
python scripts/generate_vqgan_data.py --config configs/LatentDehazingData/test_data.yaml --ckpt CKPT --outdir OUTDIR --ddpm_steps 200 --base_i 0 --seed 10000

# Structure of the generated training data

CFW_trainingdata/
    └── inputs
          └── 00000001.png # Synthesized hazy images, (512, 512, 3) 
          └── ...
    └── gts
          └── 00000001.png # Corresponding groundtruth images, (512, 512, 3) 
          └── ...
    └── latents
          └── 00000001.npy # Latent vectors (.npy) (N, 4, 64, 64) of HR images generated by the latent denoising U-Net.
          └── ...
    └── samples
          └── 00000001.png # The HR images generated from latent codes, just to make sure the generated latents are correct.
          └── ...
```
#### 